{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /Users/amritapal/opt/anaconda3/lib/python3.9/site-packages (0.1.73)\r\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /Users/amritapal/opt/anaconda3/lib/python3.9/site-packages (from contractions) (0.0.24)\r\n",
      "Requirement already satisfied: anyascii in /Users/amritapal/opt/anaconda3/lib/python3.9/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\r\n",
      "Requirement already satisfied: pyahocorasick in /Users/amritapal/opt/anaconda3/lib/python3.9/site-packages (from textsearch>=0.0.21->contractions) (2.1.0)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/amritapal/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/amritapal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/amritapal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/amritapal/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/amritapal/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "!pip3 install contractions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "import contractions\n",
    "import nltk\n",
    "\n",
    "nltk.download('words') # download list of english words\n",
    "nltk.download('stopwords') # download list of stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopWords = stopwords.words('english')\n",
    "englishWords = set(nltk.corpus.words.words())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [],
   "source": [
    "# using this link to create this project -> figuring out what libraries to use!\n",
    "# https://www.simplilearn.com/tutorials/machine-learning-tutorial/how-to-create-a-fake-news-detection-system"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [],
   "source": [
    "true_df = pd.read_csv(\"data/True.csv\")\n",
    "# true_df.head(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [],
   "source": [
    "fake_df = pd.read_csv(\"data/Fake.csv\")\n",
    "# fake_df.head(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [],
   "source": [
    "# parameters for true/false\n",
    "true_df['truthVal'] = 1\n",
    "fake_df['truthVal'] = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [],
   "source": [
    "# should manually test data at some point"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [],
   "source": [
    "merged_df = pd.concat([true_df, fake_df], axis = 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [],
   "source": [
    "# dropping unnecessary columns\n",
    "\n",
    "merged_df = merged_df.drop(['title', 'subject', 'date'], axis = 1)\n",
    "#merged_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [],
   "source": [
    "# pre-processing link to clean text: https://jon-dagdagan.medium.com/fake-news-detection-pre-processing-text-d9648a2854e5\n",
    "\n",
    "def lowercase_and_remove_URL(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_contractions(text):\n",
    "    return ' '.join([contractions.fix(word) for word in text.split()])\n",
    "\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# chose to lemmatize instead of stem\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Convert the nltk pos tags to tags (word classes) that wordnet can recognize\n",
    "def nltkToWordnet(nltk_tag):\n",
    "  if nltk_tag.startswith('J'):\n",
    "    return wordnet.ADJ\n",
    "  elif nltk_tag.startswith('V'):\n",
    "    return wordnet.VERB\n",
    "  elif nltk_tag.startswith('N'):\n",
    "    return wordnet.NOUN\n",
    "  elif nltk_tag.startswith('R'):\n",
    "    return wordnet.ADV\n",
    "  else:\n",
    "    return None # words that don't need to be reduced to their root word\n",
    "\n",
    "# Lemmatize a list of words/tokens\n",
    "def lemmatize(tokens):\n",
    "  pos_tags = nltk.pos_tag(tokens) # nltk version of word classes\n",
    "  reduced_words = []\n",
    "  for word, tag in pos_tags:\n",
    "    tag = nltkToWordnet(tag)\n",
    "    if tag is None:\n",
    "      reduced_words.append(word)\n",
    "    else:\n",
    "      reduced_words.append(lemmatizer.lemmatize(word, tag))\n",
    "  return reduced_words\n",
    "\n",
    "\n",
    "def remove_stopWords(tokens):\n",
    "    return [word for word in tokens if (word in englishWords and word not in stopWords)]\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = lowercase_and_remove_URL(text)\n",
    "    text = remove_contractions(text)\n",
    "    tokens = tokenize_text(text)\n",
    "    reduced_words = lemmatize(tokens)\n",
    "    reduced_words = remove_stopWords(reduced_words)\n",
    "    return ' '.join(reduced_words)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text  truthVal\n0  head conservative republican faction congress ...         1\n1  people allow first time enlist military starti...         1\n2  special counsel investigation link russia pres...         1\n3  trump campaign adviser tell diplomat may russi...         1\n4  president trump call postal service charge muc...         1\n5  west palm beach white house say set kick talk ...         1\n6  west palm beach president trump say believe fa...         1\n7  following statement post twitter account presi...         1\n8  following statement post twitter account presi...         1\n9  secretary state say certify democratic senator...         1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>truthVal</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>head conservative republican faction congress ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>people allow first time enlist military starti...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>special counsel investigation link russia pres...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>trump campaign adviser tell diplomat may russi...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>president trump call postal service charge muc...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>west palm beach white house say set kick talk ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>west palm beach president trump say believe fa...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>following statement post twitter account presi...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>following statement post twitter account presi...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>secretary state say certify democratic senator...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleaning merged_df text\n",
    "merged_df['text'] = merged_df['text'].apply(clean_text)\n",
    "merged_df.head(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [],
   "source": [
    "# the code above took an extremely long time to run! (note for the future as I improve code)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
